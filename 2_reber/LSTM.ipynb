
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Long Short Term Memory (LSTM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem with standard recurrent nets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standard recurrent nets, during training, will have gradients that either vanish or explode, depending on the size of the largest eigenvalue of the weight matrix. Illustration of a vanishing gradient: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"vanishing_gradient_50.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTMs and some other types of *gated* recurrent nets (GRUs among others) don't suffer from this problem. Here is how gradient would propagate through time with an LSTM:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"lstm_gradient_50.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structure of LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A single LSTM unit looks like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"lstm_diagram_50.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One could have multiple LSTM units layed out in parallel, or LSTM units on top of LSTM units, or even bidirectional LSTMs. Here is an example of a parallel layout:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"lstm_multiple_units_50.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theano Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After http://christianherta.de/lehre/dataScience/machineLearning/neuralNetworks/LSTM.php"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "\n",
    "dtype=theano.config.floatX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"LSTM_definition_corrected_50.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# squashing of the gates should result in values between 0 and 1\n",
    "# therefore we use the logistic function\n",
    "sigma = lambda x: 1 / (1 + T.exp(-x))\n",
    "\n",
    "# for the other activation function we use the tanh\n",
    "act = T.tanh\n",
    "\n",
    "# sequences: x_t\n",
    "# prior results: h_tm1, c_tm1\n",
    "# non-sequences: W_xi, W_hi, W_ci, b_i, W_xf, W_hf, W_cf, b_f, W_xc, W_hc, b_c, \n",
    "#                W_xy, W_hy, W_cy, b_y\n",
    "def one_lstm_step(x_t, h_tm1, c_tm1, W_xi, W_hi, W_ci, b_i, W_xf, W_hf, W_cf, \n",
    "                  b_f, W_xc, W_hc, b_c, W_xy, W_ho, W_cy, b_o, W_hy, b_y):\n",
    "    i_t = sigma(theano.dot(x_t, W_xi) + theano.dot(h_tm1, W_hi) + \\\n",
    "                theano.dot(c_tm1, W_ci) + b_i)\n",
    "    f_t = sigma(theano.dot(x_t, W_xf) + theano.dot(h_tm1, W_hf) + \\\n",
    "                theano.dot(c_tm1, W_cf) + b_f)\n",
    "    c_t = f_t * c_tm1 + i_t * act(theano.dot(x_t, W_xc) + \n",
    "                                  theano.dot(h_tm1, W_hc) + b_c) \n",
    "    o_t = sigma(theano.dot(x_t, W_xo)+ theano.dot(h_tm1, W_ho) + \n",
    "                theano.dot(c_t, W_co)  + b_o)\n",
    "    h_t = o_t * act(c_t)\n",
    "    y_t = sigma(theano.dot(h_t, W_hy) + b_y) \n",
    "    return [h_t, c_t, y_t]"
   ]
  },
  {