
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Long Short Term Memory (LSTM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem with standard recurrent nets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standard recurrent nets, during training, will have gradients that either vanish or explode, depending on the size of the largest eigenvalue of the weight matrix. Illustration of a vanishing gradient: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"vanishing_gradient_50.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTMs and some other types of *gated* recurrent nets (GRUs among others) don't suffer from this problem. Here is how gradient would propagate through time with an LSTM:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"lstm_gradient_50.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structure of LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A single LSTM unit looks like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"lstm_diagram_50.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One could have multiple LSTM units layed out in parallel, or LSTM units on top of LSTM units, or even bidirectional LSTMs. Here is an example of a parallel layout:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"lstm_multiple_units_50.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theano Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After http://christianherta.de/lehre/dataScience/machineLearning/neuralNetworks/LSTM.php"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "\n",
    "dtype=theano.config.floatX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"LSTM_definition_corrected_50.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# squashing of the gates should result in values between 0 and 1\n",
    "# therefore we use the logistic function\n",
    "sigma = lambda x: 1 / (1 + T.exp(-x))\n",
    "\n",
    "# for the other activation function we use the tanh\n",
    "act = T.tanh\n",