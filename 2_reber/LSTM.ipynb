
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Long Short Term Memory (LSTM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem with standard recurrent nets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standard recurrent nets, during training, will have gradients that either vanish or explode, depending on the size of the largest eigenvalue of the weight matrix. Illustration of a vanishing gradient: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"vanishing_gradient_50.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTMs and some other types of *gated* recurrent nets (GRUs among others) don't suffer from this problem. Here is how gradient would propagate through time with an LSTM:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"lstm_gradient_50.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structure of LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A single LSTM unit looks like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"lstm_diagram_50.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One could have multiple LSTM units layed out in parallel, or LSTM units on top of LSTM units, or even bidirectional LSTMs. Here is an example of a parallel layout:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"lstm_multiple_units_50.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theano Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After http://christianherta.de/lehre/dataScience/machineLearning/neuralNetworks/LSTM.php"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "\n",
    "dtype=theano.config.floatX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"LSTM_definition_corrected_50.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# squashing of the gates should result in values between 0 and 1\n",
    "# therefore we use the logistic function\n",
    "sigma = lambda x: 1 / (1 + T.exp(-x))\n",
    "\n",
    "# for the other activation function we use the tanh\n",
    "act = T.tanh\n",
    "\n",
    "# sequences: x_t\n",
    "# prior results: h_tm1, c_tm1\n",
    "# non-sequences: W_xi, W_hi, W_ci, b_i, W_xf, W_hf, W_cf, b_f, W_xc, W_hc, b_c, \n",
    "#                W_xy, W_hy, W_cy, b_y\n",
    "def one_lstm_step(x_t, h_tm1, c_tm1, W_xi, W_hi, W_ci, b_i, W_xf, W_hf, W_cf, \n",
    "                  b_f, W_xc, W_hc, b_c, W_xy, W_ho, W_cy, b_o, W_hy, b_y):\n",
    "    i_t = sigma(theano.dot(x_t, W_xi) + theano.dot(h_tm1, W_hi) + \\\n",
    "                theano.dot(c_tm1, W_ci) + b_i)\n",
    "    f_t = sigma(theano.dot(x_t, W_xf) + theano.dot(h_tm1, W_hf) + \\\n",
    "                theano.dot(c_tm1, W_cf) + b_f)\n",
    "    c_t = f_t * c_tm1 + i_t * act(theano.dot(x_t, W_xc) + \n",
    "                                  theano.dot(h_tm1, W_hc) + b_c) \n",
    "    o_t = sigma(theano.dot(x_t, W_xo)+ theano.dot(h_tm1, W_ho) + \n",
    "                theano.dot(c_t, W_co)  + b_o)\n",
    "    h_t = o_t * act(c_t)\n",
    "    y_t = sigma(theano.dot(h_t, W_hy) + b_y) \n",
    "    return [h_t, c_t, y_t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_weights(sizeX, sizeY):\n",
    "    values = np.ndarray([sizeX, sizeY], dtype=dtype)\n",
    "    for dx in xrange(sizeX):\n",
    "        values[dx,:] = np.random.uniform(low=-1., high=1., size=(sizeY,))\n",
    "    _, svs, _ = np.linalg.svd(values)\n",
    "    #svs[0] is the largest singular value                      \n",
    "    values = values / svs[0]\n",
    "    return values  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"lstm_pooling.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_in = 7 # for embedded reber grammar\n",
    "n_hidden = n_i = n_c = n_o = n_f = 10\n",
    "n_y = 7 # for embedded reber grammar\n",
    "\n",
    "# initialize weights\n",
    "# i_t and o_t should be \"open\" or \"closed\"\n",
    "# f_t should be \"open\" (don't forget at the beginning of training)\n",
    "# we try to archive this by appropriate initialization of the corresponding biases \n",
    "\n",
    "W_xi = theano.shared(sample_weights(n_in, n_i))  \n",
    "W_hi = theano.shared(sample_weights(n_hidden, n_i))  \n",
    "W_ci = theano.shared(sample_weights(n_c, n_i))  \n",
    "b_i = theano.shared(np.cast[dtype](np.random.uniform(-0.5,.5,size = n_i)))\n",
    "W_xf = theano.shared(sample_weights(n_in, n_f)) \n",
    "W_hf = theano.shared(sample_weights(n_hidden, n_f))\n",
    "W_cf = theano.shared(sample_weights(n_c, n_f))\n",
    "b_f = theano.shared(np.cast[dtype](np.random.uniform(0, 1.,size = n_f)))\n",
    "W_xc = theano.shared(sample_weights(n_in, n_c))  \n",
    "W_hc = theano.shared(sample_weights(n_hidden, n_c))\n",
    "b_c = theano.shared(np.zeros(n_c, dtype=dtype))\n",
    "W_xo = theano.shared(sample_weights(n_in, n_o))\n",
    "W_ho = theano.shared(sample_weights(n_hidden, n_o))\n",
    "W_co = theano.shared(sample_weights(n_c, n_o))\n",
    "b_o = theano.shared(np.cast[dtype](np.random.uniform(-0.5,.5,size = n_o)))\n",
    "W_hy = theano.shared(sample_weights(n_hidden, n_y))\n",
    "b_y = theano.shared(np.zeros(n_y, dtype=dtype))\n",
    "\n",
    "c0 = theano.shared(np.zeros(n_hidden, dtype=dtype))\n",
    "h0 = T.tanh(c0)\n",
    "\n",
    "params = [W_xi, W_hi, W_ci, b_i, W_xf, W_hf, W_cf, b_f, W_xc, W_hc, b_c,\n",
    "          W_xo, W_ho, W_co, b_o, W_hy, b_y, c0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#first dimension is time\n",
    "\n",
    "#input \n",
    "v = T.matrix(dtype=dtype)\n",
    "\n",
    "# target\n",
    "target = T.matrix(dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",